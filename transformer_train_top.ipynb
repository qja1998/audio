{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Xi Chen\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# Borrowed from https://github.com/neocxi/pixelsnail-public and ported it to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vq_vae_2_half_tmb import Model\n",
    "from transformer import VQVAETransformer\n",
    "from lr_scheduler import WarmupLinearLRSchedule\n",
    "from torchvision import utils as vutils\n",
    "#from utils import load_data, plot_images\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqja1998\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gibeom/audio/emotional_annotations/wandb/run-20221110_005511-3tsw0def</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/qja1998/audio-emotional_annotations/runs/3tsw0def\" target=\"_blank\">dashing-voice-68</a></strong> to <a href=\"https://wandb.ai/qja1998/audio-emotional_annotations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb.run.name = 'transformer_bot'\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelData(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        genre = ['classical', 'rock', 'electronic', 'pop']\n",
    "        \n",
    "        for g in genre:\n",
    "            for i in range(1, 101):\n",
    "                for j in range(5):\n",
    "                    tmp_path = f'{file_path}/{g}/{i}-{j}.csv'\n",
    "                    try:\n",
    "                        self.data.append((pd.read_csv(tmp_path), g, i, j))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"{g}-{i}-{j} file is deleted\")\n",
    "                        continue\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mel, g, i, j = self.data[idx]\n",
    "        mel = torch.from_numpy(pd.get_dummies(mel).values)\n",
    "        mel = mel.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        return (g, i, j), mel\n",
    "\n",
    "class EmotionalData(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tmp_data = self.data.iloc[idx]\n",
    "        genre, idx = tmp_data[0].split('_')\n",
    "        emo = tmp_data[1:]\n",
    "        return idx, genre, torch.FloatTensor(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classical-16-0 file is deleted\n",
      "classical-16-1 file is deleted\n",
      "classical-16-2 file is deleted\n",
      "classical-16-3 file is deleted\n",
      "classical-16-4 file is deleted\n",
      "classical-40-0 file is deleted\n",
      "classical-40-1 file is deleted\n",
      "classical-40-2 file is deleted\n",
      "classical-40-3 file is deleted\n",
      "classical-40-4 file is deleted\n",
      "classical-57-0 file is deleted\n",
      "classical-57-1 file is deleted\n",
      "classical-57-2 file is deleted\n",
      "classical-57-3 file is deleted\n",
      "classical-57-4 file is deleted\n",
      "classical-66-0 file is deleted\n",
      "classical-66-1 file is deleted\n",
      "classical-66-2 file is deleted\n",
      "classical-66-3 file is deleted\n",
      "classical-66-4 file is deleted\n",
      "classical-73-0 file is deleted\n",
      "classical-73-1 file is deleted\n",
      "classical-73-2 file is deleted\n",
      "classical-73-3 file is deleted\n",
      "classical-73-4 file is deleted\n"
     ]
    }
   ],
   "source": [
    "EMO_PATH = \"./mean_data.csv\"\n",
    "MEL_ARR_PATH = \"./split_mel_array\"\n",
    "SAVE_PATH = \"./save_models\"\n",
    "    \n",
    "mel_arr_data = MelData(MEL_ARR_PATH)\n",
    "#emo_data = EmotionalData(EMO_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_arr_data_loader = DataLoader(\n",
    "        dataset=mel_arr_data, batch_size=batch_size)\n",
    "\n",
    "#emo_data_loader = DataLoader(\n",
    "#        dataset=emo_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled(x):\n",
    "    return x + 80.0\n",
    "def unscaled(x):\n",
    "    return x - 80.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract idices\n",
    "# torch.Size([32, 10, 128]) torch.Size([32, 20, 256])\n",
    "def extract_indice(mel_data, model):\n",
    "    with torch.no_grad():\n",
    "        for _, mel in mel_data:\n",
    "            x = scaled(mel)\n",
    "            x = x[:, :, :-4].unsqueeze(1).to(device)\n",
    "            _, _, _, ids = model.encode(x)\n",
    "            try:\n",
    "                ids_t = torch.cat([ids_t, ids[0]], dim=0)\n",
    "                ids_m = torch.cat([ids_m, ids[1]], dim=0)\n",
    "                ids_b = torch.cat([ids_b, ids[2]], dim=0)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                ids_t, ids_m, ids_b = ids[0].clone().detach(), ids[1].clone().detach(), ids[2].clone().detach()\n",
    "    return ids_t, ids_m, ids_b\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 128 #128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 4\n",
    "embedding_dim = 64 #64\n",
    "num_embeddings = 512 #512\n",
    "commitment_cost = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae = Model(num_hiddens=num_hiddens, \n",
    "                  num_residual_layers=num_residual_layers,\n",
    "                  num_residual_hiddens=num_residual_hiddens,\n",
    "                  num_embeddings=num_embeddings,\n",
    "                  embedding_dim=embedding_dim, \n",
    "                  commitment_cost=commitment_cost).to(device)\n",
    "\n",
    "score = 79.73939\n",
    "MODEL_PATH = f'{SAVE_PATH}/vqvae2_tmb-{score:.5f}_dict.pt'\n",
    "vqvae.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local variable 'ids_t' referenced before assignment\n",
      "torch.Size([1975, 10, 32]) torch.Size([1975, 20, 64]) torch.Size([1975, 40, 128])\n"
     ]
    }
   ],
   "source": [
    "ids_t, ids_m, ids_b = extract_indice(mel_arr_data_loader, vqvae)\n",
    "print(ids_t.shape, ids_m.shape, ids_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, transformer\n",
    "importlib.reload(transformer)\n",
    "from transformer import VQVAETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTransformer:\n",
    "    def __init__(self, args, data, lev):\n",
    "        self.model = VQVAETransformer(args).to(device=args.device)\n",
    "        self.optim = self.configure_optimizers()\n",
    "        self.lr_schedule = WarmupLinearLRSchedule(\n",
    "            optimizer=self.optim,\n",
    "            init_lr=1e-6,\n",
    "            peak_lr=args.learning_rate,\n",
    "            end_lr=0.,\n",
    "            warmup_epochs=10,\n",
    "            epochs=args.epochs,\n",
    "            current_step=args.start_from_epoch\n",
    "        )\n",
    "        #self.lr_schedule = AdafactorSchedule(self.optim, initial_lr=1e-2)\n",
    "\n",
    "        if args.start_from_epoch > 1:\n",
    "            self.model.load_checkpoint(args.start_from_epoch)\n",
    "            print(f\"Loaded Transformer from epoch {args.start_from_epoch}.\")\n",
    "        \n",
    "        wandb.watch(self.model)\n",
    "        self.lev = lev\n",
    "        self.train(args, data)\n",
    "\n",
    "    def train(self, args, data):\n",
    "        train_dataset = data\n",
    "        len_train_dataset = len(train_dataset)\n",
    "        step = args.start_from_epoch * len_train_dataset\n",
    "        for epoch in range(args.start_from_epoch+1, args.epochs+1):\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            with tqdm(range(len(train_dataset))) as pbar:\n",
    "                for i, (ids_t, ids_m, ids_b) in zip(pbar, train_dataset):\n",
    "                    if self.lev == 'top':\n",
    "                        imgs = ids_t\n",
    "                    elif self.lev == 'mid':\n",
    "                        imgs = ids_m\n",
    "                    elif self.lev == 'bot':\n",
    "                        imgs = ids_b\n",
    "                    imgs = imgs.to(device=args.device)\n",
    "                    logits, target = self.model(imgs)\n",
    "                    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), target.reshape(-1)).requires_grad_(True)\n",
    "                    loss.backward()\n",
    "                    if step % args.accum_grad == 0:\n",
    "                        self.optim.step()\n",
    "                        self.optim.zero_grad()\n",
    "                    step += 1\n",
    "                    pbar.set_postfix(Transformer_Loss=np.round(loss.cpu().detach().numpy().item(), 4))\n",
    "                    pbar.update(0)\n",
    "                    \n",
    "                    pbar.set_description(\n",
    "                    (\n",
    "                        f\" lr {self.optim.param_groups[0]['lr']:.6f}\\t\"\n",
    "                    )\n",
    "                    )\n",
    "                    wandb.log({\n",
    "                        \"Loss\": loss,\n",
    "                        \"Learning rate\": self.optim.param_groups[0]['lr']\n",
    "                    })\n",
    "                self.lr_schedule.step()\n",
    "            try:\n",
    "                log, sampled_imgs = self.model.log_images(imgs[0:1])\n",
    "                vutils.save_image(sampled_imgs.add(1).mul(0.5), os.path.join(\"results\", f\"{epoch}.jpg\"), nrow=4)\n",
    "                #plot_images(log)\n",
    "            except:\n",
    "                pass\n",
    "            #if epoch % args.ckpt_interval == 0:\n",
    "            #    torch.save(self.model.state_dict(), os.path.join(\"checkpoints\", f\"transformer_epoch_{epoch}.pt\"))\n",
    "            torch.save(self.model.state_dict(), os.path.join(\"checkpoints\", f\"transformer_current_{self.lev}.pt\"))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # decay, no_decay = set(), set()\n",
    "        # whitelist_weight_modules = (nn.Linear,)\n",
    "        # blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)\n",
    "        # for mn, m in self.model.transformer.named_modules():\n",
    "        #     for pn, p in m.named_parameters():\n",
    "        #         fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n",
    "        #\n",
    "        #         if pn.endswith('bias'):\n",
    "        #             no_decay.add(fpn)\n",
    "        #\n",
    "        #         elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "        #             decay.add(fpn)\n",
    "        #\n",
    "        #         elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "        #             no_decay.add(fpn)\n",
    "        #\n",
    "        # # no_decay.add('pos_emb')\n",
    "        #\n",
    "        # param_dict = {pn: p for pn, p in self.model.transformer.named_parameters()}\n",
    "        #\n",
    "        # optim_groups = [\n",
    "        #     {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 4.5e-2},\n",
    "        #     {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        # ]\n",
    "        optimizer = torch.optim.Adam(self.model.transformer.parameters(), lr=1e-6, betas=(0.9, 0.96), weight_decay=4.5e-2)\n",
    "        #optimizer = Adafactor(self.model.parameters(), lr=0.0, scale_parameter=True, relative_step=False)\n",
    "        #optimizer = Adafactor(self.model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
    "parser.add_argument('--run-name', type=str, default=None)\n",
    "parser.add_argument('--latent-dim', type=int, default=32, help='Latent dimension n_z.')\n",
    "parser.add_argument('--image-size', type=int, default=256, help='Image height and width.)')\n",
    "parser.add_argument('--num-codebook-vectors', type=int, default=8192, help='Number of codebook vectors.')\n",
    "parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar.')\n",
    "parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images.')\n",
    "parser.add_argument('--dataset-path', type=str, default='./data', help='Path to data.')\n",
    "parser.add_argument('--checkpoint-path', type=str, default='./checkpoints/last_ckpt.pt', help='Path to checkpoint.')\n",
    "parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on.')\n",
    "parser.add_argument('--batch-size', type=int, default=10, help='Batch size for training.')\n",
    "parser.add_argument('--accum-grad', type=int, default=10, help='Number for gradient accumulation.')\n",
    "parser.add_argument('--epochs', type=int, default=300, help='Number of epochs to train.')\n",
    "parser.add_argument('--start-from-epoch', type=int, default=1, help='Number of epochs to train.')\n",
    "parser.add_argument('--ckpt-interval', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--learning-rate', type=float, default=1e-4, help='Learning rate.')\n",
    "\n",
    "parser.add_argument('--sos-token', type=int, default=1025, help='Start of Sentence token.')\n",
    "\n",
    "parser.add_argument('--n-layers', type=int, default=24, help='Number of layers of transformer.')\n",
    "parser.add_argument('--dim', type=int, default=768, help='Dimension of transformer.')\n",
    "parser.add_argument('--hidden-dim', type=int, default=3072, help='Dimension of transformer.')\n",
    "parser.add_argument('--num-image-tokens', type=int, default=256, help='Number of image tokens.')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.run_name = \"<name>\"\n",
    "args.dataset_path = r\"C:\\Users\\dome\\datasets\\landscape\"\n",
    "args.checkpoint_path = r\".\\checkpoints\"\n",
    "args.n_layers = 24\n",
    "args.dim = 512\n",
    "args.hidden_dim = 3072\n",
    "args.batch_size = 4\n",
    "args.accum_grad = 25\n",
    "args.epochs = 100\n",
    "\n",
    "args.start_from_epoch = 0\n",
    "\n",
    "args.num_codebook_vectors = 511\n",
    "args.num_image_tokens = 10 * 32\n",
    "\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDsData(Dataset):\n",
    "    def __init__(self, ids_t, ids_m, ids_b):\n",
    "        self.ids_t = ids_t\n",
    "        self.ids_m = ids_m\n",
    "        self.ids_b = ids_b\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids_t)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids_t[idx], self.ids_m[idx], self.ids_b[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(ids_t)\n",
    "ids_data = IDsData(ids_t.view(length, -1), ids_m.view(length, -1), ids_b.view(length, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "ids_loader = DataLoader(\n",
    "        ids_data, batch_size=batch, shuffle=True, drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Module Embedding.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module NonDynamicallyQuantizableLinear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Initializing Module Linear.\n",
      "Transformer parameters: 101704001\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000001\t: 100%|██████████| 1975/1975 [02:44<00:00, 12.00it/s, Transformer_Loss=4.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000001\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.97it/s, Transformer_Loss=3.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000011\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.93it/s, Transformer_Loss=2.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000021\t: 100%|██████████| 1975/1975 [02:44<00:00, 11.98it/s, Transformer_Loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000031\t: 100%|██████████| 1975/1975 [02:44<00:00, 11.98it/s, Transformer_Loss=1.59]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000041\t: 100%|██████████| 1975/1975 [02:44<00:00, 11.98it/s, Transformer_Loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000050\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.97it/s, Transformer_Loss=0.665] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000060\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.94it/s, Transformer_Loss=1.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000070\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.94it/s, Transformer_Loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000080\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.95it/s, Transformer_Loss=0.506] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000090\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.91it/s, Transformer_Loss=1.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000100\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.97it/s, Transformer_Loss=0.735] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000099\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.90it/s, Transformer_Loss=0.0204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000098\t: 100%|██████████| 1975/1975 [02:44<00:00, 11.99it/s, Transformer_Loss=0.881] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000097\t: 100%|██████████| 1975/1975 [02:46<00:00, 11.88it/s, Transformer_Loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000096\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.94it/s, Transformer_Loss=0.732] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000094\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.92it/s, Transformer_Loss=0.629] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000093\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.95it/s, Transformer_Loss=0.716] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000092\t: 100%|██████████| 1975/1975 [02:45<00:00, 11.95it/s, Transformer_Loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000091\t: 100%|██████████| 1975/1975 [02:46<00:00, 11.88it/s, Transformer_Loss=0.0179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000090\t: 100%|██████████| 1975/1975 [02:46<00:00, 11.86it/s, Transformer_Loss=0.0171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " lr 0.000089\t:  82%|████████▏ | 1618/1975 [02:15<00:29, 11.91it/s, Transformer_Loss=0.454] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8334/699179824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'top'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8334/2545323973.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, data, lev)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8334/2545323973.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, args, data)\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/audio/emotional_annotations/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z_indices)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msos_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/audio/emotional_annotations/bidirectional_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# position_embeddings = self.pos_emb(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToken_Prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/audio/emotional_annotations/bidirectional_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/audio/lib/python3.10/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapproximate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproximate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_transformer = TrainTransformer(args, ids_loader, lev='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_transformer.model.state_dict(), os.path.join(\"checkpoints\", \"transformer_current_bot.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9625287269220251dd2f53164d2a112ab09cf8e86b35d6b73b22f9f410170108"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
